Daniel can point us to a few papers we can use as a basis.
We should look at the references in the proposal.
Network-based.
Different levels:
- We are caching data at big servers
- Take it all the way down to servers in the network itself.

Coded caching (flexibility in where to pick fragments). Daniel will provide papers on this.

Compare GDD to both plain old caching with full duplication and to coded caching.

Simple way: servers running on my pc with client/servers
A bit more complicated: Implement client in browser

Perhaps start with:
If I purely wanted to do deduplication, how would I go about this

For next time:
Read articles
Think about overall architecture
Envision scenario with clients trying to access N files with various commonalities across files
Think about network/data usage for this